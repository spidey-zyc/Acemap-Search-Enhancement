import requests
import pandas as pd
import json
import sys
import os
import io

# === 1. ç¯å¢ƒä¸è·¯å¾„è®¾ç½® ===
# ç¡®ä¿èƒ½å¯¼å…¥ src ç›®å½•ä¸‹çš„æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.agent import SearchAgent

# === 2. é…ç½® ===
ACEMAP_API_URL = "https://acemap.info/api/v1/work/search"
REPORT_FILE = "search_report.md"  # ç»“æœå°†ä¿å­˜åˆ°è¿™ä¸ªæ–‡ä»¶

# ç”¨äºç¼“å­˜è¾“å‡ºæ—¥å¿—ï¼Œæœ€åç»Ÿä¸€å†™å…¥æ–‡ä»¶
log_buffer = []

def log(text=""):
    """åŒæ—¶æ‰“å°åˆ°ç»ˆç«¯å’Œç¼“å­˜åˆ°æ–‡ä»¶"""
    print(text)
    log_buffer.append(text)

# ==========================================
# 3. API è°ƒç”¨ (ä¿æŒä¹‹å‰ä¿®å¤åçš„ç‰ˆæœ¬)
# ==========================================
def call_acemap_api(keyword, limit=10):
    if not keyword:
        return 0, []

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    }

    params = {
        "keyword": keyword,
        "page": 1,
        "size": limit,
        "order": "desc"
    }
    
    try:
        response = requests.get(ACEMAP_API_URL, params=params, headers=headers, timeout=10)
        if response.status_code != 200:
            return 0, []

        data = response.json()
        
        # é€‚é… Acemap æ–°ç‰ˆ JSON ç»“æ„ {"results": [...]}
        if "results" in data:
            papers = data["results"]
            total = data.get("meta", {}).get("count", len(papers))
            return total, papers
        else:
            return 0, []
    except Exception:
        return 0, []

# ==========================================
# 4. æ ¸å¿ƒå¯¹æ¯”é€»è¾‘ (å¢å¼ºç‰ˆ)
# ==========================================
def run_comparison(case_name, user_query, agent):
    log(f"\n## æµ‹è¯•åœºæ™¯: {case_name}")
    log(f"**ç”¨æˆ·æŸ¥è¯¢:** `{user_query}`\n")

    # --- åœºæ™¯ A: Before (ç›´æ¥æœåŸå¥) ---
    log("### ğŸ”´ Before: åŸå§‹æœç´¢")
    total_raw, papers_raw = call_acemap_api(user_query, limit=5)
    
    if total_raw == 0:
        log(f"> **ç»“æœ:** 0 ç¯‡ (æœç´¢å¤±è´¥)")
    else:
        log(f"> **ç»“æœ:** {total_raw} ç¯‡ (å¯èƒ½åŒ…å«æ— å…³ç»“æœ)")
        # ç®€å•å±•ç¤ºå‰2ç¯‡æ ‡é¢˜
        for i, p in enumerate(papers_raw[:2]):
            title = p.get('display_name') or p.get('title')
            log(f"- {i+1}. {title}")

    # --- åœºæ™¯ B: After (Agent å¢å¼º) ---
    log("\n### ğŸŸ¢ After: Agent å¢å¼ºæœç´¢")
    
    # 1. Agent è§£ææ„å›¾
    try:
        agent_output = agent.parse(user_query)
    except Exception as e:
        log(f"âŒ Agent Error: {e}")
        return

    # è·å–å‚æ•°
    params = agent_output.get('search_params', {})
    filters = agent_output.get('filters', {})
    
    grounded_kws = params.get('keywords_grounded', [])
    raw_kws = params.get('keywords_raw', [])
    
    # 2. é€‰è¯ç­–ç•¥
    best_keyword = user_query
    strategy = "åŸå¥å…œåº•"
    
    if grounded_kws:
        best_keyword = grounded_kws[0]
        strategy = "**KGæ ¡å‡† (Grounding)** âœ¨"
    elif raw_kws:
        best_keyword = raw_kws[0]
        strategy = "**LLMæå– (Extraction)**"
        
    log(f"- **ç­–ç•¥:** {strategy}")
    log(f"- **ä¼˜åŒ–å…³é”®è¯:** `{user_query}` -> `{best_keyword}`")
    
    # 3. API å¬å›
    # ä¸ºäº†æ¼”ç¤ºè¿‡æ»¤æ•ˆæœï¼Œæˆ‘ä»¬å¤šå¬å›ä¸€äº›æ•°æ® (limit=20)
    total_opt, papers_opt = call_acemap_api(best_keyword, limit=20)
    log(f"- **åˆæ­¥å¬å›:** {total_opt} ç¯‡")

    # 4. å®¢æˆ·ç«¯æ™ºèƒ½è¿‡æ»¤ (Client-side Filtering)
    year_start = filters.get('year_start')
    institution = filters.get('institution')
    
    final_papers = []
    
    if year_start:
        log(f"- **æ‰§è¡Œè¿‡æ»¤:** å¹´ä»½ >= {year_start}")
        for p in papers_opt:
            p_year = p.get('publication_year')
            # åªæœ‰å¹´ä»½å­˜åœ¨ä¸”ç¬¦åˆè¦æ±‚æ‰ä¿ç•™
            if p_year and int(p_year) >= int(year_start):
                final_papers.append(p)
    else:
        final_papers = papers_opt
        
    # (å¯é€‰) æœºæ„è¿‡æ»¤é€»è¾‘
    # æ³¨æ„ï¼šAcemap API è¿”å›åˆ—è¡¨é€šå¸¸ä¸å«æœºæ„ä¿¡æ¯ï¼Œè¿™é‡Œä»…åšé€»è¾‘æ¼”ç¤º
    if institution:
         log(f"- **æ„å›¾è¯†åˆ«åˆ°çš„æœºæ„:** {institution} (æ³¨: å› APIé™åˆ¶ï¼Œæœ¬æ­¥éª¤ä»…åšå±•ç¤ºï¼Œæš‚æœªæ‰§è¡Œä¸¥æ ¼è¿‡æ»¤)")

    # 5. ç”Ÿæˆç»“æœè¡¨æ ¼
    if not final_papers:
        log("> **æœ€ç»ˆæ¨è:** æ— ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
    else:
        log(f"> **âœ… æœ€ç»ˆæ¨è:** {len(final_papers)} ç¯‡ (Top 5 å±•ç¤º)")
        
        table_data = []
        for p in final_papers[:5]:
            title = p.get('display_name') or p.get('title') or "No Title"
            table_data.append({
                "Title": title[:50] + "...", # æˆªæ–­æ ‡é¢˜
                "Year": p.get('publication_year', 'N/A'),
                "Cited": p.get('cited_by_count', 0)
            })
            
        # ä½¿ç”¨ Pandas ç”Ÿæˆ Markdown è¡¨æ ¼
        df = pd.DataFrame(table_data)
        log("\n" + df.to_markdown(index=False))

    log("\n---\n")

# ==========================================
# 5. ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # åˆå§‹åŒ– Agent (åªåŠ è½½ä¸€æ¬¡ KG)
    print("ğŸš€ åˆå§‹åŒ– Agent ä¸­... (åŠ è½½ Parquet å¯èƒ½éœ€è¦å‡ ç§’)")
    agent = SearchAgent()
    
    # å‡†å¤‡æŠ¥å‘Šå¤´
    log("# Acemap Search Agent æµ‹è¯•æŠ¥å‘Š\n")
    log("æœ¬æŠ¥å‘Šå¯¹æ¯”äº†åŸå§‹æœç´¢ä¸ Agent å¢å¼ºæœç´¢åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚\n")

    # === æµ‹è¯•ç”¨ä¾‹ 1: æ‹¼å†™é”™è¯¯ä¸æœ¯è¯­æ ¡å‡† ===
    # ç›®çš„: å±•ç¤º GAKG çš„ Grounding èƒ½åŠ›
    run_comparison(
        case_name="Case 1: æ‹¼å†™é”™è¯¯çº æ­£ (KG Grounding)",
        user_query="recent papers about Grnite",
        agent=agent
    )
    
    # === æµ‹è¯•ç”¨ä¾‹ 2: å¤šæ¡ä»¶å¤æ‚é€»è¾‘ ===
    # ç›®çš„: å±•ç¤º LLM çš„æ„å›¾æå– + å®¢æˆ·ç«¯å¹´ä»½è¿‡æ»¤
    run_comparison(
        case_name="Case 2: å¤æ‚æ„å›¾ä¸æ—¶é—´è¿‡æ»¤ (Logic & Filtering)",
        user_query="Find papers on Basalt from 2023",
        agent=agent
    )

    # === æµ‹è¯•ç”¨ä¾‹ 3: è·¨è¯­è¨€æ£€ç´¢ ===
    # ç›®çš„: å±•ç¤º LLM å°†ä¸­æ–‡å£è¯­è½¬åŒ–ä¸ºè‹±æ–‡å­¦æœ¯æœ¯è¯­çš„èƒ½åŠ›
    run_comparison(
        case_name="Case 3: è·¨è¯­è¨€/ä¸“ä¸šæœ¯è¯­æ˜ å°„ (Translation)",
        user_query="å¸®æˆ‘æ‰¾å…³äºæ¿å—æ„é€ çš„è®ºæ–‡", 
        # Agent ä¼šå°†å…¶ç¿»è¯‘ä¸º "Plate tectonics"
        agent=agent
    )

    # === æµ‹è¯•ç”¨ä¾‹ 4: æœ¯è¯­æ¶ˆæ­§/ç¼©å†™è¿˜åŸ (å¯é€‰) ===
    # å¦‚æœä½ çš„å›¾è°±é‡Œæœ‰ MORB -> Mid-Ocean Ridge Basalt çš„å…³ç³»
    run_comparison(
        case_name="Case 4: æœ¯è¯­ç¼©å†™è¿˜åŸ (Normalization)",
        user_query="Papers about MORB", 
        agent=agent
    )

    # ä¿å­˜æŠ¥å‘Š
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(log_buffer))
    
    print(f"âœ… æµ‹è¯•å®Œæˆï¼å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {os.path.abspath(REPORT_FILE)}")